
# Q-Learning ‚Äì Projeto de Aprendizado por Refor√ßo com Amongois

Este projeto implementa o algoritmo **Q-Learning** para treinar um agente capaz de controlar o personagem **Amongois** em um jogo. O objetivo √© que o agente aprenda, por tentativa e erro, o melhor caminho at√© o **bloco preto**, evitando cair e recebendo recompensas de acordo com suas a√ß√µes.

---

## Como Executar o Projeto

### 1. Inicie o Jogo

- Execute o arquivo `.exe` do jogo Amongois.
- Verifique a **porta TCP** exibida na janela do jogo (ex: `2037`).

> Essa porta deve ser usada no script de treinamento para estabelecer a conex√£o.

### 2. Execute o Script de Treinamento

- No terminal, com o ambiente Python ativado e o jogo aberto, execute:

```bash
python client.py
```

Esse script:

- Conecta ao jogo via socket.
- Treina o agente a partir de epis√≥dios sucessivos.
- Armazena a Q-table aprendida no arquivo `resultado.txt`.
- Encerra o treinamento automaticamente quando todos os **96 estados** forem visitados ou quando atingir o n√∫mero m√°ximo de epis√≥dios.

> A Q-table √© salva automaticamente a cada 10 epis√≥dios e ao final do treinamento.

---

## Como Funciona o Q-Learning

O Q-Learning √© um algoritmo de **aprendizado por refor√ßo**. O agente interage com o ambiente, executa a√ß√µes e aprende com base nas recompensas recebidas.

### F√≥rmula de atualiza√ß√£o (Equa√ß√£o de Bellman):

```
new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
```

- `alpha`: taxa de aprendizado
- `gamma`: fator de desconto
- `reward`: recompensa recebida

A Q-table guarda os valores esperados de cada a√ß√£o em cada estado, e √© usada para decidir as a√ß√µes futuras.

---

## Estrat√©gia de Aprendizado

- **Estados**: representados por n√∫meros inteiros de 0 a 95 (convertidos do bin√°rio recebido do jogo).
- **A√ß√µes dispon√≠veis:** `"left"`, `"right"`, `"jump"`
- **Recompensas:**
  - `-1 a -14`: penalidades por movimenta√ß√µes comuns
  - `300`: objetivo alcan√ßado
  - `-100`: morte (queda)

### Pol√≠tica Epsilon-Greedy

- A√ß√£o aleat√≥ria com probabilidade `epsilon` (explora√ß√£o).
- Melhor a√ß√£o segundo a Q-table com probabilidade `1 - epsilon` (explora√ß√£o).
- `epsilon` decai a cada passo at√© o m√≠nimo de `0.05`, para garantir aprendizado est√°vel e ampla cobertura de estados.

---

## üóÇÔ∏è Arquivos do Projeto

| Arquivo                      | Descri√ß√£o |
|------------------------------|-----------|
| `client.py`                  | Script de treinamento e preenchimento da Q-table |
| `connection.py`              | Interface de comunica√ß√£o com o jogo via socket TCP |
| `resultado.txt`              | Q-table contendo os valores aprendidos para cada estado e a√ß√£o |

---

## ‚öôÔ∏è Hiperpar√¢metros

| Par√¢metro         | Valor  |
|-------------------|--------|
| `alpha`           | 0.6    |
| `gamma`           | 0.9    |
| `epsilon` inicial | 0.3    |
| `min_epsilon`     | 0.05   |
| `decay_rate`      | 0.995  |
| `epis√≥dios m√°x.`  | 5000   |

---

## Resultados

- A Q-table aprendida cobre os 96 estados poss√≠veis do ambiente.
- O agente √© capaz de alcan√ßar o objetivo ap√≥s treinamento suficiente.

---

## Entreg√°veis

- `client.py`
- `connection.py`
- `resultado.txt` (Q-table final)
- v√≠deo demonstrando o projeto.

---

## Grupo

Pedro de Andrade Lima Fischer de Lyra
Ivo Luiz Soares Neto
Walter Crasto Monteiro
Daniel Dias Fernandes
