import random
import numpy as np
import time
import json
from connection import connect, get_state_reward

# ==========================
# ðŸ”§ HiperparÃ¢metros Otimizados para Velocidade
# ==========================
ALPHA = 0.1           # Taxa de aprendizado mais alta (aprendizado mais rÃ¡pido)
GAMMA = 0.9           # Fator de desconto
EPSILON_START = 0.5   # Taxa de exploraÃ§Ã£o inicial (menor)
EPSILON_END = 0.05    # Taxa de exploraÃ§Ã£o final
EPSILON_DECAY = 0.99  # Decaimento mais rÃ¡pido do epsilon
NUM_EPISODES = 1000    # NÃºmero reduzido de episÃ³dios
MAX_STEPS = 100       # MÃ¡ximo de passos por episÃ³dio (reduzido)
PORTA = 2037          # Porta do jogo

# ==========================
# ðŸŽ¯ ConfiguraÃ§Ãµes do ambiente
# ==========================
NUM_STATES = 96       # 24 plataformas Ã— 4 direÃ§Ãµes
NUM_ACTIONS = 3       # left, right, jump
ACTIONS = ["left", "right", "jump"]

# Mapeamento de aÃ§Ãµes para Ã­ndices
ACTION_TO_INDEX = {action: i for i, action in enumerate(ACTIONS)}

# ==========================
# ðŸ“Š Classe para estatÃ­sticas
# ==========================
class TrainingStats:
    def __init__(self):
        self.episode_rewards = []
        self.episode_steps = []
        self.success_count = 0
        self.epsilon_history = []
    
    def add_episode(self, total_reward, steps, success, epsilon):
        self.episode_rewards.append(total_reward)
        self.episode_steps.append(steps)
        self.epsilon_history.append(epsilon)
        if success:
            self.success_count += 1
    
    def print_stats(self, episode):
        if episode % 25 == 0 and episode > 0:  # Stats mais frequentes
            recent_rewards = self.episode_rewards[-25:] if len(self.episode_rewards) >= 25 else self.episode_rewards
            avg_reward = np.mean(recent_rewards)
            avg_steps = np.mean(self.episode_steps[-25:]) if len(self.episode_steps) >= 25 else np.mean(self.episode_steps)
            success_rate = (self.success_count / episode) * 100
            print(f"ðŸ“Š Ep {episode}: Avg Reward: {avg_reward:.1f} | Steps: {avg_steps:.0f} | Success: {success_rate:.1f}% | Îµ: {self.epsilon_history[-1]:.2f}")

# ==========================
# ðŸ§  Classe Q-Learning Agent
# ==========================
class QLearningAgent:
    def __init__(self, num_states, num_actions, alpha=ALPHA, gamma=GAMMA):
        self.num_states = num_states
        self.num_actions = num_actions
        self.alpha = alpha
        self.gamma = gamma
        self.q_table = np.zeros((num_states, num_actions))
        self.epsilon = EPSILON_START
        
    def state_to_index(self, state_binary):
        """
        Converte estado binÃ¡rio para Ã­ndice (0-95).
        Valida se o estado estÃ¡ no range correto.
        """
        try:
            index = int(state_binary, 2)
            if 0 <= index < self.num_states:
                return index
            else:
                print(f"âš ï¸ Estado fora do range: {index}")
                return 0  # Estado padrÃ£o
        except ValueError:
            print(f"âš ï¸ Estado invÃ¡lido: {state_binary}")
            return 0
    
    def choose_action(self, state_index):
        """
        Escolhe aÃ§Ã£o usando estratÃ©gia Îµ-greedy.
        """
        if random.random() < self.epsilon:
            # ExploraÃ§Ã£o: aÃ§Ã£o aleatÃ³ria
            return random.choice(ACTIONS)
        else:
            # ExploraÃ§Ã£o: melhor aÃ§Ã£o conhecida
            best_action_index = np.argmax(self.q_table[state_index])
            return ACTIONS[best_action_index]
    
    def update_q_value(self, state, action, reward, next_state):
        """
        Atualiza Q-table usando a fÃ³rmula do Q-Learning.
        """
        state_idx = self.state_to_index(state)
        next_state_idx = self.state_to_index(next_state)
        action_idx = ACTION_TO_INDEX[action]
        
        # Q-Learning: Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
        current_q = self.q_table[state_idx, action_idx]
        max_next_q = np.max(self.q_table[next_state_idx])
        
        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)
        self.q_table[state_idx, action_idx] = new_q
    
    def decay_epsilon(self):
        """
        Reduz epsilon gradualmente (menos exploraÃ§Ã£o, mais exploraÃ§Ã£o).
        """
        self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)
    
    def save_q_table(self, filename="resultado.txt"):
        """
        Salva Q-table no formato especificado:
        - Apenas dados numÃ©ricos
        - Ordenados por estado (0-95)
        - Colunas: [left, right, jump]
        """
        try:
            with open(filename, "w") as file:
                for state_idx in range(self.num_states):
                    row = self.q_table[state_idx]
                    # Formatar com 6 casas decimais para precisÃ£o
                    line = " ".join([f"{value:.6f}" for value in row])
                    file.write(line + "\n")
            print(f"âœ… Q-table salva em '{filename}'")
            return True
        except Exception as e:
            print(f"âŒ Erro ao salvar Q-table: {e}")
            return False
    
    def load_q_table(self, filename="resultado.txt"):
        """
        Carrega Q-table de arquivo (Ãºtil para continuar treinamento).
        """
        try:
            self.q_table = np.loadtxt(filename)
            print(f"âœ… Q-table carregada de '{filename}'")
            return True
        except Exception as e:
            print(f"âš ï¸ NÃ£o foi possÃ­vel carregar Q-table: {e}")
            return False

# ==========================
# ðŸŽ® FunÃ§Ã£o principal de treinamento
# ==========================
def train_agent():
    """
    Treina o agente Q-Learning no jogo Amongois.
    """
    print("ðŸš€ Iniciando treinamento RÃPIDO do Q-Learning Agent")
    print(f"ðŸ“‹ ConfiguraÃ§Ãµes otimizadas para velocidade:")
    print(f"   EpisÃ³dios: {NUM_EPISODES} (reduzido)")
    print(f"   Max steps: {MAX_STEPS} (reduzido)")
    print(f"   Alpha: {ALPHA} (mais alto)")
    print(f"   Epsilon: {EPSILON_START} â†’ {EPSILON_END} (decaimento rÃ¡pido)")
    print("=" * 50)
    
    # Inicializar agente e estatÃ­sticas
    agent = QLearningAgent(NUM_STATES, NUM_ACTIONS)
    stats = TrainingStats()
    
    # Tentar carregar Q-table existente
    agent.load_q_table()
    
    for episode in range(1, NUM_EPISODES + 1):
        # Conectar ao jogo
        socket_conn = connect(PORTA)
        if socket_conn == 0:
            print("âŒ Falha na conexÃ£o. Verifique se o jogo estÃ¡ rodando.")
            break
        
        # Obter estado inicial
        try:
            initial_state, _ = get_state_reward(socket_conn, "jump")
            current_state = initial_state
        except Exception as e:
            print(f"âŒ Erro ao obter estado inicial: {e}")
            socket_conn.close()
            continue
        
        # VariÃ¡veis do episÃ³dio
        total_reward = 0
        steps = 0
        episode_success = False
        
        # Loop do episÃ³dio
        for step in range(MAX_STEPS):
            # Escolher e executar aÃ§Ã£o
            action = agent.choose_action(agent.state_to_index(current_state))
            
            try:
                next_state, reward = get_state_reward(socket_conn, action)
            except Exception as e:
                print(f"âŒ Erro na comunicaÃ§Ã£o: {e}")
                break
            
            # Atualizar Q-table
            agent.update_q_value(current_state, action, reward, next_state)
            
            # Atualizar mÃ©tricas
            total_reward += reward
            steps += 1
            
            # Verificar condiÃ§Ãµes de parada
            if reward == -1:  # Chegou ao objetivo
                episode_success = True
                if episode % 25 == 0:  # SÃ³ mostra sucesso a cada 25 episÃ³dios
                    print(f"ðŸŽ‰ Ep {episode}: SUCESSO em {steps} passos!")
                break
            elif reward <= -12:  # Penalidade muito alta (mais restritiva)
                break
            
            current_state = next_state
        
        # Fechar conexÃ£o
        socket_conn.close()
        
        # Decair epsilon
        agent.decay_epsilon()
        
        # Registrar estatÃ­sticas
        stats.add_episode(total_reward, steps, episode_success, agent.epsilon)
        stats.print_stats(episode)
        
        # Salvar Q-table periodicamente (menos frequente)
        if episode % 50 == 0:
            agent.save_q_table(f"checkpoint_ep{episode}.txt")
    
    # Salvar Q-table final
    agent.save_q_table()
    
    # EstatÃ­sticas finais
    print("\nðŸ TREINAMENTO CONCLUÃDO!")
    print(f"ðŸ“Š EstatÃ­sticas finais:")
    print(f"   Total de sucessos: {stats.success_count}/{NUM_EPISODES}")
    print(f"   Taxa de sucesso: {(stats.success_count/NUM_EPISODES)*100:.1f}%")
    print(f"   Recompensa mÃ©dia: {np.mean(stats.episode_rewards):.2f}")
    print(f"   Epsilon final: {agent.epsilon:.3f}")
    
    return agent

# ==========================
# ðŸŽ¯ FunÃ§Ã£o para testar agente treinado
# ==========================
def test_agent(num_tests=5):  # Menos testes
    """
    Testa o agente jÃ¡ treinado (sem exploraÃ§Ã£o) - versÃ£o rÃ¡pida.
    """
    print("ðŸ§ª Testando agente treinado (teste rÃ¡pido)...")
    
    agent = QLearningAgent(NUM_STATES, NUM_ACTIONS)
    agent.epsilon = 0  # Sem exploraÃ§Ã£o
    
    if not agent.load_q_table():
        print("âŒ NÃ£o foi possÃ­vel carregar Q-table para teste.")
        return
    
    successes = 0
    
    for test in range(1, num_tests + 1):
        socket_conn = connect(PORTA)
        if socket_conn == 0:
            continue
        
        try:
            current_state, _ = get_state_reward(socket_conn, "jump")
            steps = 0
            
            for step in range(MAX_STEPS):
                action = agent.choose_action(agent.state_to_index(current_state))
                next_state, reward = get_state_reward(socket_conn, action)
                
                steps += 1
                if reward == -1:
                    successes += 1
                    print(f"âœ… Teste {test}: Sucesso em {steps} passos")
                    break
                elif reward <= -12:
                    print(f"âŒ Teste {test}: Falha em {steps} passos")
                    break
                
                current_state = next_state
                # Removida a pausa para teste mais rÃ¡pido
        
        except Exception as e:
            print(f"âŒ Erro no teste {test}: {e}")
        
        socket_conn.close()
    
    print(f"ðŸŽ¯ Resultado dos testes: {successes}/{num_tests} sucessos ({(successes/num_tests)*100:.1f}%)")

# ==========================
# ðŸš€ ExecuÃ§Ã£o principal
# ==========================
if __name__ == "__main__":
    try:
        # Treinar agente
        trained_agent = train_agent()
        
        # Perguntar se quer testar
        test_choice = input("\nðŸ¤” Deseja testar o agente treinado? (s/n): ")
        if test_choice.lower() == 's':
            test_agent()
    
    except KeyboardInterrupt:
        print("\nâ¸ï¸ Treinamento interrompido pelo usuÃ¡rio.")
    except Exception as e:
        print(f"\nâŒ Erro durante execuÃ§Ã£o: {e}")
